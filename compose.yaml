services:
  llama1b-llm:
    image: vllm/vllm-openai:latest
    command: >-
      --model meta-llama/Llama-3.2-1B-Instruct --host 0.0.0.0 --port 8000 --max-model-len 10000 --gpu-memory-utilization 0.2
    ports:
      - "9999:8000"
    env_file:
      - .env
    volumes:
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '0' ]
              capabilities: [ gpu ]
    healthcheck:
      test: curl --fail http://localhost:8000/health || exit 1
      interval: 10s
      timeout: 30s
      retries: 3
      start_period: 90s
  llama8b-prm:
    image: vllm/vllm-openai:latest
    command: >-
      --model RLHFlow/Llama3.1-8B-PRM-Deepseek-Data --host 0.0.0.0 --port 8000 --max-model-len 10000 --gpu-memory-utilization 0.75
    ports:
      - "8888:8000"
    env_file:
      - .env
    volumes:
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
    # depends_on:
    #   llama1b-llm:
    #     condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '1' ]
              capabilities: [ gpu ]
    healthcheck:
      test: curl --fail http://localhost:8000/health || exit 1
      interval: 10s
      timeout: 30s
      retries: 3
      start_period: 90s
    #   app:
    #     image: kungfu.azurecr.io/scepsy:latest
    #     env_file:
    #       - .env
    #     working_dir: /app/scepsy-dev
    #     volumes:
    #       - ./:/app/scepsy-dev
    #       - ${HOME}/.cache/huggingface:/root/.cache/huggingface
    #     network_mode: host
    #     depends_on:
    #       - vllm
    #     command: >-
    #       bash -lc "set -euo pipefail && pip3 install --editable . && python3 examples/langchain/beam_search/beam_search_langchain.py"
    #

  qwen-embedding-8b:
    image: vllm/vllm-openai:latest
    command: >-
      --model Qwen/Qwen3-Embedding-8B --host 0.0.0.0 --port 8000 --task embed
    ports:
      - "7777:8000"
    env_file:
      - .env
    volumes:
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '0' ]
              capabilities: [ gpu ]
    healthcheck:
      test: curl --fail http://localhost:8000/health || exit 1
      interval: 10s
      timeout: 30s
      retries: 3
      start_period: 90s
    profiles:
      - donotstart
