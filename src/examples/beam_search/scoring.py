from datasets import Dataset

from src.sal.qwen_math_parser import extract_answer, math_equal

def create_dataset_from_results(dataset_list: list, answer_thoughts: list) -> Dataset:
    """Transform beam search results into dataset format matching beam_search_async.py"""
    updated_samples = []
    for problem_data, problem_thought_dicts in zip(dataset_list, answer_thoughts):
        # Create completions and scores from thoughts
        completions = []
        scores = []
        
        for thought_dict in problem_thought_dicts:
            # Join all steps into a single completion text
            completion_text = "\n\n".join(thought_dict["steps"])
            completions.append(completion_text)
            # Use the step-wise scores
            scores.append(thought_dict["scores"])
        
        # Find best completion based on last score (matching beam_search_async logic)
        if completions:
            best_idx = max(range(len(problem_thought_dicts)), key=lambda i: problem_thought_dicts[i]["scores"][-1] if problem_thought_dicts[i]["scores"] else 0)
            pred = completions[best_idx]
        else:
            pred = ""
        
        # Create updated sample with required fields
        updated_sample = dict(problem_data)  # Copy original data
        updated_sample["completions"] = completions
        updated_sample["scores"] = scores  
        updated_sample["pred"] = pred
        updated_samples.append(updated_sample)

    return Dataset.from_list(updated_samples)

def calculate_and_print_accuracies(scored_dataset: Dataset) -> None:
    """Calculate and print accuracy metrics for all prediction fields generated by the score function"""
    
    # Find all prediction fields in the dataset
    sample = scored_dataset[0]
    pred_fields = [key for key in sample.keys() if key.startswith('pred_')]
    
    # Group by N value and sort for organized display
    pred_groups: dict[int, dict[str, str]] = {}
    for field in pred_fields:
        if '@' in field:
            strategy, n_str = field.split('@')
            n = int(n_str)
            if n not in pred_groups:
                pred_groups[n] = {}
            pred_groups[n][strategy] = field
    
    print("\n" + "="*80)
    print("ACCURACY METRICS BY NUMBER OF COMPLETIONS")
    print("="*80)
    
    # Sort by N value (powers of 2: 1, 2, 4, 8, ...)
    for n in sorted(pred_groups.keys()):
        print(f"\nUsing Top {n} Completion{'s' if n > 1 else ''}:")
        print("-" * 40)
        
        # Display in consistent order: weighted, majority, naive
        strategy_order = ['pred_weighted', 'pred_maj', 'pred_naive']
        strategy_names = {'pred_weighted': 'Weighted', 'pred_maj': 'Majority', 'pred_naive': 'Naive (Best)'}
        
        for strategy in strategy_order:
            if strategy in pred_groups[n]:
                pred_field = pred_groups[n][strategy]
                correct_count = 0
                total_count = 0
                
                for sample in scored_dataset:
                    # Get correct answer
                    correct_answer = str(sample["answer"])  # type: ignore
                    
                    # Get prediction
                    prediction = sample.get(pred_field)  # type: ignore
                    if prediction:
                        pred_str = str(prediction)
                        
                        # Extract answer from prediction using the robust prebuilt function
                        predicted_answer = extract_answer(pred_str, "math")
                        
                        # Use the robust math_equal function for comparison
                        if math_equal(predicted_answer, correct_answer):
                            correct_count += 1
                            
                    total_count += 1
                
                # Calculate and print accuracy
                if total_count > 0:
                    accuracy = (correct_count / total_count) * 100
                    strategy_name = strategy_names[strategy]
                    print(f"  {strategy_name:<12}: {accuracy:6.2f}% ({correct_count}/{total_count})")
                else:
                    print(f"  {strategy_names[strategy]:<12}: No valid predictions")
    
    print("\n" + "="*80)